## tokenizer.py说明  

直接执行可以比对transformers版本和Numpy版本，后者不依赖transformers库和torch  

执行结果：  
```commandline
transformers库 encode 结果: [3223, 94844, 93307, 93865, 92386, 95043, 92528, 93747, 93747, 65, 59783, 92453, 6278, 45451, 94278]
百川2 transformers库和torch库 tokenizer 计算结果：
tensor([[  195,  3223, 94844, 93307, 93865, 92386, 95043, 92528, 93747, 93747,
            65, 59783, 92453,  6278, 45451, 94278,   196]])
----------------------------------------------
百川2 mumpy tokenizer 计算结果：
numpy版本直接 encode 结果: [3223, 94844, 93307, 93865, 92386, 95043, 92528, 93747, 93747, 65, 59783, 92453, 6278, 45451, 94278]
[[  195  3223 94844 93307 93865 92386 95043 92528 93747 93747    65 59783
  92453  6278 45451 94278   196]]
```

显示两个版本计算结果一致  

Numpy版本注意点：  

1. 目前来看 两个版本都应该没有什么需要优化速度的地方，原理就是去词典中取词index,速度优化应该在后续的cpp版本中体现  
2. sentencepiece库目前来看应该使用了swig编译技术，由C/CPP编译为python接口，后期cpp调用应该不需要编译swig了，直接用cpp的so（官方仓库地址：https://github.com/google/sentencepiece ）  
3. tokenizer对象没有自己封装，只是简单实现了拆解。后面的`model.chat`才是令人头疼的事儿
